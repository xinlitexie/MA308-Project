---
title: "网易云歌单数据分析报告"
author: "zhaozixi，wangyuean"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = TRUE)
# 设置中文字体，防止绘图乱码
windowsFonts(SimHei = windowsFont("SimHei"))
```

# 1. 项目背景与研究意义

在算法推荐时代，歌单不仅是音乐的集合，更是情绪、场景与身份的混合体，是平台、创作者与听众三方博弈的核心载体。

本次研究旨在利用公开数据拆解播放量背后的因果链，帮助平台发现高潜力内容，协助创作者低成本试错，并为用户提供过滤噪音的依据。

# 2. 环境加载与数据预处理

**数据概况：**
原始数据集包含 2048 个歌单样本，涵盖79个特征变量。我们将首先进行缺失值处理（主要是“简介”字段）以及将分类变量转化为二值变量。

```{r load_libraries_and_data}
library(tidyverse)
library(ggplot2)
library(cluster)
library(lubridate)
library(naniar)
library(corrplot)
library(jiebaRD)
library(wordcloud)
library(scatterplot3d)
library(MASS)
library(scales)
library(dplyr)
library(stringr)
library(randomForest)
library(broom)
library(car)
library(moments)
library(nnet)
library(caret)
library(reshape2)
library(lmPerm)

data <- read_csv("网易云音乐歌单分析/data.csv")

# ==== 1、清洗数据 ====

###获取数据基础信息
names(data)
str(data)
cat("数据维度：", dim(data), "\n")
colSums(is.na(data))
gg_miss_var(data) + labs(title = "变量缺失值分布")

###填充缺失值，因为只有introduction部分有缺失值，所以用"[无简介]"替代原本null内容
data$introduction[is.na(data$introduction) | data$introduction == ""] <- "[无简介]"

###将所有只由是否组成的数据转变为factor类型
convert_binary_proper <- function(data) {
  for(col_name in names(data)) {
    if(is.character(data[[col_name]]) || is.factor(data[[col_name]])) {
      unique_vals <- unique(data[[col_name]])
      
      # 处理"是"/"否"的情况
      if(length(unique_vals) == 2 && all(sort(unique_vals) == c("否", "是"))) {
        # 创建因子，但更重要的是转换为0/1
        data[[paste0(col_name, "_binary")]] <- ifelse(
          data[[col_name]] == "是", 1, 0
        )
        cat("已将列", col_name, "转换为二值变量（是=1，否=0）\n")
      }
    }
  }
  return(data)
}
data <- convert_binary_proper(data) # 注意：原代码此处调用名可能有误，已修正为定义的函数名

###删除处理后的列
remove_columns_tidy <- function(data, col_names) {
  existing_cols <- col_names[col_names %in% names(data)]
  missing_cols <- col_names[!col_names %in% names(data)]
  
  if(length(missing_cols) > 0) {
    warning("以下列不存在于数据中: ", paste(missing_cols, collapse = ", "))
  }
  
  if(length(existing_cols) > 0) {
    data <- data %>% dplyr::select(-all_of(existing_cols))
    cat("已删除列:", paste(existing_cols, collapse = ", "), "\n")
  }
  return(data)
}

data_cleaned = data
```

# 3. 描述性统计与词云分析

**分析结论：**
* **长尾分布**：播放量数据呈现显著的长尾分布（偏度约8.969987），大部分歌单播放量较低。后续分析中已对播放量进行了 `log1p` 变换。
* **文本分析**：通过词云发现，高价值标签主要体现核心价值用户（80后、90后），使用场景化、情绪化语言。

```{r descriptive_stats, fig.height=8, fig.width=10}
# ==== 2、描述性分布和热图 ====

### 提取numeric和factor变量，并进行简单数据统计分析
numeric_vars <- names(data_cleaned)[sapply(data_cleaned, is.numeric)]
factor_vars <- names(data_cleaned)[sapply(data_cleaned, is.factor)]
desc_stats <- (data_cleaned[, numeric_vars])
summary(desc_stats)

###绘制统计数据图片
par(mfrow = c(2, 1), mar = c(2, 3, 1, 2) + 0.1)

for (var in numeric_vars) {
  current_data = data_cleaned[[var]]
  kernels <- c("rectangular", "triangular", "epanechnikov", "gaussian")
  colors <- c("red", "blue", "green", "yellow")
  hist(current_data, freq = FALSE, main = paste("直方图与核密度估计:", var), xlab = var, ylab = "密度", col = "lightgray", border = "black", ylim = c(0, max(hist(current_data, plot = FALSE)$density) * 1.3))
  
  bandwidths = 1
  for (i in seq_along(kernels)) {
    kernel_type <- kernels[i]
    color <- colors[i]
    kde <- density(current_data, kernel = kernel_type, bandwidths = bandwidths, adjust = 1)
    lines(kde, col = color, lty = 1, lwd = 2)
  }
  
  legend("topright", legend = kernels, col = colors, lty = 1, lwd = 2, title = "核函数", bty = "n")
  
  boxplot(current_data, main = paste("箱线图:", var), ylab = var, col = "lightblue")
}
par(mfrow = c(1, 1))
```

```{r factor_stats, fig.height=6, fig.width=12}
###对于factor变量的基础统计
# 如果 factor_vars 为空，此段代码不会产生输出
if(length(factor_vars) > 0){
  par(mfrow = c(1, 3), mar = c(4, 4, 3, 2))
  
  for (var in factor_vars) {
    freq_data <- table(data_cleaned[[var]])
    prop_data <- prop.table(freq_data)
    
    categories <- names(freq_data)
    colors <- c("#1f77b4", "#ff7f0e")  # 两个颜色
    
    barplot(freq_data, 
            main = paste("频数分布:", var),
            xlab = var,
            ylab = "频数",
            col = colors[1:length(categories)],
            border = "black",
            ylim = c(0, max(freq_data) * 1.2))
    
    text(x = seq_along(freq_data), 
         y = freq_data,
         label = freq_data,
         pos = 3,  # 在条形上方
         cex = 0.8,
         col = "black")
    
    barplot(prop_data * 100, 
            main = paste("百分比分布:", var),
            xlab = var,
            ylab = "百分比 (%)",
            col = colors[1:length(categories)],
            border = "black",
            ylim = c(0, 100))
    
    text(x = seq_along(prop_data), 
         y = prop_data * 100,
         label = paste0(round(prop_data * 100, 1), "%"),
         pos = 3,
         cex = 0.8,
         col = "black")
    
    pie(freq_data,
        main = paste("饼图:", var),
        col = colors[1:length(categories)],
        labels = paste0(categories, "\n", 
                        freq_data, " (", 
                        round(prop_data * 100, 1), "%)"),
        cex = 0.9)
    
    legend("topright",
           legend = categories,
           fill = colors[1:length(categories)],
           title = paste("水平:", var))
  }
  par(mfrow = c(1, 1))
}
```

```{r text_analysis_cloud}
###设置中文字体
windowsFonts(SimHei = windowsFont("SimHei"))

###定义停用词列表
stopwords_custom <- c("br", "介绍", "一个", "没有", "可以", "一种", "就是", "因为", "一些", "这个", "这些", "不是", "所以", "什么", "那些")

###清洗文本中所有的标点
text_clean <- function (texts) {
  # 增加对NA的处理，防止报错
  texts[is.na(texts)] <- ""
  all_text <- paste(texts, collapse = " ")
  all_text <- str_replace_all(all_text, "[，。]", " ")
  return(all_text)
}

###分析提取热词并绘图
text_analysis <- function(texts) {
  text_cleaned <- text_clean(texts)
  
  ###初始化分词器
  cutter <- worker()
  words <- cutter[text_cleaned]
  words_filtered <- words[
    nchar(words) > 1 &
      !words %in% stopwords_custom
  ]
  word_freq <- table(words_filtered) %>%
    as.data.frame() %>%
    arrange(desc(Freq)) %>%
    head(50)
  colnames(word_freq) <- c("Word", "Frequency")
  
  ###绘制词云图
  tryCatch({
    wordcloud(words = word_freq$Word,
            freq = word_freq$Frequency, 
            min.freq = 5,
            max.words = 100,
            random.order = FALSE,
            colors = rainbow(10),
            family = "SimHei")
  }, error = function(e) {message("词云绘制跳过: 数据不足或画布太小")})
  
  ###返回高频词以及高频词的频率
  return(word_freq)
}

# 运行名字和简介分析
name_analysis <- text_analysis(data_cleaned$name)
introduction_analysis <- text_analysis(data_cleaned$introduction)

###是否需要log变换
skewness(data_cleaned$play_count)
```

# 4. 核心指标的皮尔逊相关性统计分析 (模块一)

基于皮尔逊相关系数矩阵（Pearson Correlation
Matrix）的热力图结果，我们对播放量（play_count）与各核心变量之间的线性依赖关系进行分析

**分析结论：**
1. 收藏量与播放量呈现极强正相关性 数据结果显示，收藏量（collect_count）
与播放量的相关系数在所有变量中最高（颜色最深）。

2. 互动指标的协同效应显著 分享量（share_count） 与 评论数（comment_count） 均与播放量保持显著的正向线性关系，且这三者（收藏、分享、评论）之间存在明显的多重共线性（Multicollinearity）。

3. 作者粉丝数的相关性强度分析 作者粉丝数（fans） 与播放量呈正相关，但根据热力图色块强度对比，其相关系数低于收藏量和分享量。

4. 综上所述，播放量的增长与用户深度互动指标（特别是收藏量）存在强统计关联。

```{r module_1_corr, fig.width=8, fig.height=8}
# ====模块1：探究播放量、收藏量、分享量、评论量以及作者粉丝数之间的线性相关关系。 ====
result_vars <- data_cleaned %>%
  dplyr::select(play_count, collect_count, share_count, comment_count, fans)
# 计算相关系数矩阵
cor_matrix <- cor(result_vars, use = "complete.obs")
# 绘制热力图
corrplot(cor_matrix, 
         method = "color", 
         type = "upper", 
         addCoef.col = "black", # 显示相关系数数值
         tl.col = "black",      # 标签颜色
         tl.srt = 45,           # 标签旋转角度
         title = "Correlation Matrix", 
         mar = c(0,0,1,0))
```

# 5. 基于标签分类的播放量均值差异分析 (模块二)

通过对歌单标签进行清洗与拆解，并计算各标签下歌单的平均播放量 ，我们观察到不同风格与主题的标签在流量获取上存在显著的 层级分化

**模块二分析结论：**

1. 头部标签的集中趋势 统计结果显示，部分特定标签（如“华语”、“流行”、“90后”等）的样本平均播放量远高于全局均值。

2. 尾部标签的长尾效应 相对地，部分小众风格标签（如特定器乐、冷门流派）的平均播放量显著较低。

3. 标签频率与热度的非线性关系 对比“标签出现频率”与“标签平均播放量”可以发现，高频使用的标签并不一定对应最高的平均播放量（即“热门标签”不等于“高流量标签”）。

```{r module_2_tags}
#====模块2：高价值标签挖掘====
tag_analysis_top <- data_cleaned %>%
  dplyr::select(topics, play_count) %>%
  mutate(topics = str_remove_all(topics, "\\[|\\]|'|\"| ")) %>%
  separate_rows(topics, sep = ",") %>%
  # 按标签分组统计
  group_by(topics) %>%
  summarise(
    avg_play = mean(play_count),
    count = n()
  ) %>%
  # 按平均播放量降序排列
  arrange(desc(avg_play)) %>%
  slice_head(n = 10)

tag_analysis_bottom <- data_cleaned %>%
  dplyr::select(topics, play_count) %>%
  mutate(topics = str_remove_all(topics, "\\[|\\]|'|\"| ")) %>%
  separate_rows(topics, sep = ",") %>%
  # 按标签分组统计
  group_by(topics) %>%
  summarise(
    avg_play = mean(play_count),
    count = n()
  ) %>%
  # 按平均播放量升序排列
  arrange(avg_play) %>%
  slice_head(n = 10)

tag_analysis_avoid_extreme_top <- data_cleaned %>%
  dplyr::select(topics, play_count) %>%
  mutate(topics = str_remove_all(topics, "\\[|\\]|'|\"| ")) %>%
  separate_rows(topics, sep = ",") %>%
  # 按标签分组统计
  group_by(topics) %>%
  summarise(
    avg_play = mean(play_count),
    count = n()
  ) %>%
  filter(count > 10) %>%
  # 按平均播放量降序排列
  arrange(desc(avg_play)) %>%
  slice_head(n = 10)

tag_analysis_avoid_extreme_bottom <- data_cleaned %>%
  dplyr::select(topics, play_count) %>%
  mutate(topics = str_remove_all(topics, "\\[|\\]|'|\"| ")) %>%
  separate_rows(topics, sep = ",") %>%
  # 按标签分组统计
  group_by(topics) %>%
  summarise(
    avg_play = mean(play_count),
    count = n()
  ) %>%
  filter(count > 10) %>%
  # 按平均播放量升序排列
  arrange(avg_play) %>%
  slice_head(n = 10)

tag_freq <- data_cleaned %>%
  dplyr::select(topics, play_count) %>%
  mutate(topics = str_remove_all(topics, "\\[|\\]|'|\"| ")) %>%
  separate_rows(topics, sep = ",") %>%
  # 按标签分组统计
  group_by(topics) %>%
  summarise(
    count = n()
  ) %>%
  arrange(desc(count)) %>%
  slice_head(n = 10)

# 绘图
ggplot(tag_analysis_top, aes(x = reorder(topics, avg_play), y = avg_play)) +
  geom_col(fill = "steelblue") +
  coord_flip() + # 翻转坐标轴便于阅读标签
  labs(title = "Top 10 Tags by Average Play Count", x = "Tags", y = "Average Play Count") +
  theme_minimal()

ggplot(tag_analysis_bottom, aes(x = reorder(topics, avg_play), y = avg_play)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Bottom 10 Tags by Average Play Count", x = "Tags", y = "Average Play Count") +
  theme_minimal()

ggplot(tag_analysis_avoid_extreme_top, aes(x = reorder(topics, avg_play), y = avg_play)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Tags by Average Play Count(Freq > 10)", x = "Tags", y = "Average Play Count") +
  theme_minimal()

ggplot(tag_analysis_avoid_extreme_bottom, aes(x = reorder(topics, avg_play), y = avg_play)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Bottom 10 Tags by Average Play Count(Freq > 10)", x = "Tags", y = "Average Play Count") +
  theme_minimal()

ggplot(tag_freq, aes(x = reorder(topics, count), y = count)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Tag frequency", x = "Tags", y = "Top 10 Tags by Tag Frequency") +
  theme_minimal()
```
# 6. 标签对播放量的多元回归系数显著性检验 (模块三)

为了量化不同标签对播放量的独立影响，剔除其他干扰因素，我们构建了多元线性回归模型 log(play_count) ~ tags。基于模型生成的 回归系数置信区间图

**模块三分析结论：**

1.显著的正向驱动因子回归系数图显示，位于 $X=0$ 轴右侧的红色节点代表对播放量有显著正向贡献的标签
标签华语和散步的系数估计值显著为正。这表明在控制其他变量不变的情况下，包含这些标签的歌单具有更高的预期播放量。这些标签是当前平台生态中的“流量密码”，具有极高的边际收益

2.显著的负向抑制因子位于 $X=0$轴左侧的蓝色节点代表与播放量呈负相关的标签。标签蓝调和 NewAge的系数显著为负。负向因子与置信区间相反，部分标签（如“蓝调”、“民族”等）的回归系数为负，且置信区间（Confidence Interval）不包含0。这些标签往往对应小众或冷门垂直领域。虽然它们可能精准命中特定长尾人群，但在统计层面，它们对大众流量的获取存在显著的抑制作用

3.置信区间与稳健性分析显著性判断：半数高影响力标签的置信区间并未跨越 0轴，证明其对播放量的影响在 $\alpha=0.05$ 水平下是统计显著的，而非随机波动。其它（如“经典”、“日语”等）
```{r module_3_tag_regression, fig.height=10}
#====模块3：多元线性回归进一步分析tag impact====
###提取并集标签
all_tags <- unique(c(
  tag_analysis_avoid_extreme_top$topics,
  tag_analysis_avoid_extreme_bottom$topics,
  tag_freq$topics
))

cat("共提取", length(all_tags), "个不重复标签\n")

###清洗topic量
model_data_long <- data_cleaned %>%
  dplyr::select(play_count, topics) %>%
  mutate(
    topics_clean = str_remove_all(topics, "\\[|\\]|'|\"| "),
    log_play = log1p(play_count)
  ) %>%
  separate_rows(topics_clean, sep = ",") %>%
  filter(topics_clean != "")

###获取唯一歌曲记录
unique_songs <- model_data_long %>% distinct(play_count, log_play)
tag_matrix <- matrix(0L, 
                     nrow = nrow(unique_songs), 
                     ncol = length(all_tags))
colnames(tag_matrix) <- paste0("tag_", make.names(all_tags))

# 填充矩阵
for(i in seq_len(nrow(unique_songs))) {
  song_play_count <- unique_songs$play_count[i]
  song_tags <- model_data_long$topics_clean[model_data_long$play_count == song_play_count]
  
  for(tag in song_tags) {
    if(tag %in% all_tags) {
      col_idx <- match(make.names(tag), make.names(all_tags))
      tag_matrix[i, col_idx] <- 1L
    }
  }
}
model_data_wide <- cbind(unique_songs, as.data.frame(tag_matrix))

###线性回归
feature_cols <- colnames(tag_matrix)
formula_str <- paste("log_play ~", paste(feature_cols, collapse = " + "))
model_lm <- lm(as.formula(formula_str), data = model_data_wide)

###提取系数
model_all_tags <- tidy(model_lm, conf.int = TRUE) %>%
  filter(term %in% feature_cols) %>%
  mutate(
    tag_name = all_tags[match(str_remove(term, "tag_"), make.names(all_tags))],
    significance = ifelse(p.value < 0.05, "Significant", "Not Significant")
  )

# 绘制基本图
ggplot(model_all_tags, aes(x = estimate, y = reorder(tag_name, estimate), color = ifelse(estimate > 0, "Positive", "Negative"), alpha = significance)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = c("Positive" = "#E41A1C", "Negative" = "#377EB8")) +
  scale_alpha_manual(values = c("Significant" = 1.0, "Not Significant" = 0.4)) +
  labs(title = "All Tags Impact on Play Count",
       subtitle = "Linear Regression Coefficients with 95% CI | Red = Positive, Blue = Negative",
       x = "Effect Size (Impact on Log Play Count)",
       y = NULL,
       color = "Impact Direction",
       alpha = "Significance") +
  theme_minimal()
```

# 7. 基于残差诊断的异常值识别与数据清洗 (模块四)

为了识别数据集中可能扭曲后续聚类结果的极端样本，我们构建了以对数播放量为因变量，互动指标为自变量的诊断性线性回归模型（Diagnostic OLS Model）：log_play ~ collect + share + comment

**分析结论：**

1. 诊断模型的统计显著性与局限性
整体显著性：F 统计量为 283.9 ($p < 2.2e^{-16}$)，表明互动指标整体上能显著解释播放量的变异
拟合优度：$R^2 = 0.294$，说明仅由互动指标构建的线性模型只能解释约 29.4% 的播放量方差。这暗示播放量的形成机制存在非线性特征或受其他未观测变量（如算法推荐权重、发布时间）影响，导致存在较大的残差（Residuals），这正是我们检测异常值的统计基础

2. 回归系数的结构性分析
收藏量（collect_count）：系数为 $3.085e^{-05}$ ($t=12.48, p<0.001$)，是模型中最强的正向预测因子，再次印证了“收藏”是流量转化的核心。
分享量（share_count）：系数呈现反直觉的负值 ($-8.037e^{-04}$)。考虑到模块一中分享量与播放量呈正相关，此处的负系数极有可能是由于变量间存在高度 多重共线性所致（即收藏与分享高度相关，模型在控制收藏量后，分享量的边际效应发生了符号翻转）。在异常值诊断场景下，我们主要关注残差，故可暂时容忍此共线性

3. 异常值与高杠杆点的识别结果
高影响力异常值（Outliers, n=47）：识别出 47 个样本，其 Cook's Distance 超过阈值。这些点通常代表“数据录入错误”或“极端的头部爆款”（流量远超模型预测值的超级歌单）
高杠杆点（High Leverage Points, n=74）：识别出 74 个样本在自变量空间（互动数据）上偏离均值过远

4. 数据清洗策略
为了保证后续 K-Means 聚类算法的鲁棒性，我们在进入模块五之前，决定将这 47 个高影响力异常值
从训练集中剔除
```{r module_4_outliers, fig.height=8}
#====模块4：检测播放数异常值点和杠杆点====

data_cleaned <- data_cleaned %>%
  # 使用log1p处理0值
  mutate(log_play = log1p(play_count)) %>%
  filter(play_count > 0, collect_count >= 0, share_count >= 0, comment_count >= 0) %>%
  na.omit()

###构建model
diagnostic_formula <- log_play ~ collect_count + share_count + comment_count
diag_model <- lm(diagnostic_formula, data = data_cleaned)
summary(diag_model)

###计算诊断统计量
par(mar = c(6, 6, 5, 3) + 0.1, mgp = c(3.5, 1, 0))
cooks_dist <- cooks.distance(diag_model)
leverage <- hatvalues(diag_model)
std_resid <- rstandard(diag_model)
p_influence <- influencePlot(
  diag_model,
  main = "影响力诊断图 (Influence Plot)",
  sub = "圆圈大小 ∝ Cook's距离" ,
  xlab = "标准化残差",
  ylab = "杠杆值",
  col = c("steelblue", "orange", "red"),
  pch = c(16, 18),
  lwd = 1.5
)

threshold_cook <- 4 / nrow(data_cleaned)
threshold_leverage <- 3 * length(coef(diag_model)) / nrow(data_cleaned)

###识别异常值
outlier_mask <- cooks_dist > threshold_cook
leverage_mask <- leverage > threshold_leverage

# out_vals <- outlier_mask[outlier_mask == TRUE] # 仅作为变量存储
cat("识别出异常值:", sum(outlier_mask), "个\n")
cat("识别出高杠杆值:", sum(leverage_mask), "个\n")
```
# 8. 基于 K-Means 的歌单流量分层模型 (模块五)

在剔除了模块四识别出的高影响力异常值（Outliers）后，我们对剩余样本进行了 K-Means 聚类分析。通过肘部法则（Elbow Method）确定最佳聚类数 $k=4$，并计算了组间平方和（Between-Cluster Sum of Squares, BCSS）与总平方和（Total Sum of Squares, TSS）的比值

**分析结论：**

1. 聚类模型的解释力
模型结果显示，解释方差比例高达 89.53%。这一指标（$BCSS/TSS$）衡量了聚类结果对数据变异的解释程度。接近 90%
的数值表明，仅通过将歌单划分为 4 个层级，就能解释整个数据集中绝大部分的播放量差异。这意味着歌单的流量分布并非均匀的连续体，而是存在显著的阶梯状分层结构

2. 异常值剔除的必要性验证
鲁棒性分析：在未剔除异常值的前测中，由于极少数“超级爆款”拉扯了聚类中心，导致大部分腰部和尾部歌单被压缩在同一个低值簇中，掩盖了数据的内部结构
优化结果：剔除异常值后的高解释方差证明，剩余的 99% 样本（普通歌单）有着清晰的**“头部-腰部-尾部”**金字塔结构。这种处理方式让模型对中低流量层的区分度显著提升

3. 四层流量结构的商业定义
基于 $k=4$ 的聚类中心（Centroids）排序，我们可以将歌单生态定义为：
Cluster 1 (长尾层)：播放量极低，对应无效投稿或自嗨型内容，数量庞大。
Cluster 2 (潜力层)：拥有一定基础流量，属于腰部潜力股，是算法挖掘的重点区域。
Cluster 3 (热门层)：平台的主流推荐内容，具有较高的商业价值。
Cluster 4 (准爆款层)：接近但未达到异常值（超级爆款）水平的头部内容。
```{r module_5_clustering}
#====模块5：聚类分析（需要去掉异常值点），之后再补充进去====
###提取正常值
data_normal <- data_cleaned %>% filter(!outlier_mask) %>% dplyr::select(log_play, play_count)
data_outliers <- data_cleaned %>% filter(outlier_mask) %>% dplyr::select(log_play, play_count) %>% mutate(cluster = NA)

###肘部算法
set.seed(1234)
wss = numeric(10)
for (i in 1:10) {
  km = kmeans(data_normal$play_count, centers = i, nstart = 25)
  wss[i] = km$tot.withinss
}
plot(1:10, wss, type = "b", xlab = "Number of groups", ylab = "Within groups sum of squares", main = "播放量肘部算法确定聚类数")

kmeans_function <- function(data, col_names, k) {
  existing_cols <- col_names[col_names %in% names(data)]
  if(length(existing_cols) > 0) {
    data_subset <- data[, existing_cols, drop = FALSE]
    set.seed(123)
    kmeans_result <- kmeans(data_subset, centers = k, nstart = 25)
    cluster_centers <- kmeans_result$centers
    sorted_centers <- cluster_centers[order(cluster_centers[, 1]), , drop = FALSE]
    data$cluster <- kmeans_result$cluster
  } else {
    stop("没有找到任何有效的列进行聚类分析。")
  }
  return(list(kmeans_result = kmeans_result, data_with_clusters = data))
}

###导出结果
result1 <- kmeans_function(data_normal, "play_count", k = 4)
result2 <- kmeans_function(data_cleaned, "play_count", k = 4)

###可视化操作
data_clustered1 <- result1$data_with_clusters
ggplot(data_clustered1, aes(x = factor(cluster), y = play_count, fill = factor(cluster))) +
  geom_boxplot() +
  labs(title = "不同聚类的播放量分布（去除异常值）", x = "聚类", y = "播放次数") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()

data_clustered2 <- result2$data_with_clusters
ggplot(data_clustered2, aes(x = factor(cluster), y = play_count, fill = factor(cluster))) +
  geom_boxplot() +
  labs(title = "不同聚类的播放量分布（不去除异常值）", x = "聚类", y = "播放次数") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()

###解释方差比例
explained_variance1 <- result1$kmeans_result$betweenss / result1$kmeans_result$totss
cat("去除异常值后解释方差比例: ", round(explained_variance1 * 100, 2), "%\n")
```

# 9. 基于日均播放量的生命周期与流量效率评估(模块六)

为了消除歌单发布时间长短对总播放量造成的累积偏差（即“老歌单优势”），我们构建了 “日均播放量（Daily Play Velocity）” 指标（$Total Plays / Days Alive$），以此衡量歌单获取流量的真实效率与当前热度

**分析结论：**

1. 头部歌单的流量特征分析 根据 top_velocity 列表数据，排名前列的歌单展现出极高的流量获取效率：
榜首歌单 《沉浸于字里行间°读书专用背景音乐》 实现了约 29.36万次/日
的惊人播放增速。这表明“功能性音乐”（如读书、健身、睡眠）具有极高的用户复听率和长尾效应，是平台流量的稳定基石。
排名第三的 《「前奏控」被前奏秒杀的古风歌》（约19.9万次/日）验证了“黄金前奏（Hook）”对于留存听众的关键作用。
对比数据发现，榜首歌单的粉丝数（13.8万）远低于第三名（191.8万），但其日均播放量却更高。这不仅打破了“唯粉丝论”，更在统计上证明了优质的内容定位比作者粉丝基数更能决定流量效率

2. 象限分析法的战略价值 通过以中位数为界构建的“存活天数-日均播放量”四象限图，我们将歌单划分为四类生命周期状态：
第一象限（右上：老但优秀）：发布时间长且日均播放量依然维持高位
第二象限（左上：新生优秀）：此类歌单发布时间短但日均播放量高（高于中位数）
第三象限（左下：新生滞涨）：发布时间较短且未能获得初始流量
第四象限（右下：长尾沉寂）：虽然歌单老，但缺乏每日活跃听众，流量获取效率极低

```{r module_6_lifecycle}
#====模块6：日均播放量====
reference_date <- max(data_cleaned$create_time)

data_velocity <- data_cleaned %>%
  mutate(
    # 计算歌单存活天数
    days_alive = as.numeric(difftime(reference_date, create_time, units = "days")),
    # 计算日均播放量 
    plays_per_day = play_count / ifelse(days_alive==0, 1, days_alive)
  ) %>%
  arrange(desc(plays_per_day))

top_velocity <- head(data_velocity %>% dplyr::select(name, plays_per_day, days_alive, fans), 10)
print(top_velocity)

median_days <- median(data_velocity$days_alive)
median_plays <- median(data_velocity$plays_per_day)
data_velocity$quadrant <- with(data_velocity, {
  case_when(
    days_alive <= median_days & plays_per_day > median_plays ~ "左上：新生优秀",
    days_alive <= median_days & plays_per_day <= median_plays ~ "左下：新生不优秀",
    days_alive > median_days & plays_per_day > median_plays ~ "右上：老但优秀",
    days_alive > median_days & plays_per_day <= median_plays ~ "右下：老不优秀"
  ) %>% factor(levels = c("左上：新生优秀", "左下：新生不优秀", "右上：老但优秀", "右下：老不优秀"))
})

ggplot(data_velocity, aes(x = days_alive, y = plays_per_day, color = quadrant)) +
  geom_point(alpha = 0.7, size = 2.5) +
  geom_vline(xintercept = median_days, linetype = "dashed", color = "gray50", size = 1) +
  geom_hline(yintercept = median_plays, linetype = "dashed", color = "gray50", size = 1) +
  scale_y_log10(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = "日均播放量四象限分析",
    subtitle = paste("分界线: 天数=", round(median_days), "天, 播放量=", round(median_plays)),
    x = "存活天数", y = "日均播放量",
    color = "象限类型"
  ) +
  scale_color_manual(values = c(
    "左上：新生优秀" = "#2ECC71",
    "左下：新生不优秀" = "#E74C3C",
    "右上：老但优秀" = "#3498DB",
    "右下：老不优秀" = "#95A5A6"
  )) +
  theme_minimal()
```

# 10.歌单形态特征的显著性筛选(模块七)

基于逐步回归（StepAIC）的模型优选过程，我们对歌单的基础形态特征（标题长度、简介长度、歌曲数量、热门歌手数）进行了筛选

**分析结论：**

1. 冗余变量剔除 模型比较（ANOVA）显示，包含“歌曲总数”的全模型与简化模型无显著差异。这意味着在统计层面，歌单包含多少首歌并不会直接影响播放量。用户并不在意歌单的“容量”，而更在意其“质量”或“描述”

2. 关键形态指标 最终保留的变量（标题长度、简介长度、热门歌手数）均表现出显著性。这验证了 SEO（搜索引擎优化） 在歌单运营中的作用。更详尽的标题和简介包含了更多的关键词，增加了歌单在站内搜索中被命中的概率。同时，“热门歌手”作为流量钩子，能显著提升点击转化率

```{r module_7}
#====模块7：歌单信息的影响====
data_morph <- data_cleaned %>%
  dplyr::select(play_count, length_name, length_intro, number_songs, number_hot_singers) %>%
  filter(play_count > 0) %>%
  na.omit()

###公式：log播放量 ~ 标题长度 + 简介长度 + 歌曲总数 + 热门歌手数
model_morph <- lm(log(play_count) ~ length_name + length_intro + number_songs + number_hot_singers, data = data_morph)
stepAIC(model_morph, direction = "backward")
model_morph_changed <- lm(log(play_count) ~ length_name + length_intro + number_hot_singers, data = data_morph)
anova(model_morph_changed, model_morph)
summary(model_morph_changed)
```

# 11.作者效应的交互作用与稀释效应(模块八)

我们构建了包含交互项的多元回归模型 log(play_count) ~ fans * grade * playlists + talent * verification，$R^2=0.1241$，F统计量显著。模型揭示了作者特征对播放量的非线性影响机制

**分析结论：**

1. 数量的“稀释效应”
变量 playlists（作者累计发布的歌单数）的系数显著为负 ($-1.213e^{-02}, p < 0.001$)。
这是一个反直觉但深刻的发现。在控制了粉丝和等级后，发布越多的歌单反而对应越低的平均播放量。这表明“题海战术”在网易云生态中是失效的。大量发布低质内容会稀释作者的权重，导致算法推荐的边际收益递减

2. 达人身份的独立显著性
talent是（达人）的主效应系数显著为正 ($0.317, p < 0.001$)，而 verification是（认证）的主效应不显著 ($p=0.816$)。在回归模型中，“达人”身份是比“认证”更稳健的流量预测因子。这可能因为“达人”是平台基于活跃度和内容质量赋予的动态标签，而“认证”更多代表社会身份，后者不一定能转化为平台内的流量。

3. 交互效应与边际效用递减
talent:verification（达人 × 认证）：交互项系数显著为负 ($-1.242, p < 0.05$)。
fans:grade（粉丝 × 等级）：交互项显著为正 ($p < 0.05$)。
```{r module_8}
#====模块8：作者信息的影响====
data_author <- data_cleaned %>%
  dplyr::select(play_count, fans, grade, playlists, talent, verification, musician) %>%
  filter(play_count > 0) %>%
  na.omit()

# BoxCox检查 (注释掉以加快RMD生成速度，如需要可取消注释)
# boxcox(lm(play_count ~ fans * grade * playlists + talent * verification * musician, data = data_author), 
#        lambda = seq(-2, 2, length.out = 100))

model_full <- lm(
  log(play_count) ~ fans * grade * playlists + talent * verification * musician,
  data = data_author
)
summary(model_full)
model_changed <- stepAIC(model_full, direction = "backward")
summary(model_changed)
anova(model_changed, model_full)
```

# 12.身份价值的非参数置换检验(模块九)

为了进一步验证不同作者身份（达人、认证、音乐人）对播放量的真实提升效果，我们采用了对异常值不敏感的 置换检验

**分析结论：**

1. “达人”身份的稳健性优势
Talent vs None 的比较中，均值差异为 +24.5万，P值极度显著 ($p < 0.001$)
“达人”身份是唯一在统计学上具有绝对显著性的流量提升门槛。无论样本如何重抽样，达人歌单的播放量始终显著高于普通用户

2. “认证”身份的高方差特征
Verification vs None 的均值差异高达 +53.3万（远高于达人），但 P值仅为 $0.025$（显著性弱于达人）
巨大的均值差异与较弱的 P值并存，说明“认证”群体内部存在极大的方差。该群体中可能包含少数自带巨大流量的明星（拉高了均值），但也包含大量无人问津的无效认证号。相比之下，“达人”的流量表现更为稳定和普适

3. “音乐人”身份的无效性
Musician vs None 的 P值为 $0.120$（大于 0.05）
无法拒绝原假设。统计数据表明，“音乐人”身份对于歌单播放量没有显著的提升作用。这可能是因为音乐人更多关注原创单曲的发布，而非歌单整理，导致其在歌单领域的算法权重并不高于普通用户
```{r module_9}
#====模块9：身份的作用(用permutation test来做检测)====
data_id <- data_cleaned %>%
  mutate(
    is_talent = ifelse(talent == "是", 1, 0),
    is_verified = ifelse(verification == "是", 1, 0),
    is_musician = ifelse(musician == "是", 1, 0)
  )

# 构建 4 级身份因子
data_id <- data_id %>%
  mutate(
    identity_4level = case_when(
      is_verified == 1 ~ "Verification",
      is_talent == 1 ~ "Talent",
      is_musician == 1 ~ "Musician",
      TRUE ~ "None"
    ) %>% factor(levels = c("None", "Musician", "Talent", "Verification")))

comparison_pairs <- list(
  c("None", "Talent"),
  c("None", "Musician"),
  c("None", "Verification"),
  c("Musician", "Talent"),
  c("Talent", "Verification"),
  c("Musician", "Verification")
)

run_coin_test <- function(data, group_col, value_col, pair, n_perm = 1000) { # n_perm 减小以加快演示
  sub_data <- data %>%
    filter(!!sym(group_col) %in% pair) %>%
    dplyr::select(!!sym(group_col), !!sym(value_col)) %>%
    na.omit()
  
  sub_data[[group_col]] <- factor(sub_data[[group_col]], levels = pair)
  
  if(nrow(sub_data) < 4 || length(unique(sub_data[[group_col]])) < 2) {
    return(NULL)
  }
  
  group1_vec <- sub_data[[value_col]][sub_data[[group_col]] == pair[1]]
  group2_vec <- sub_data[[value_col]][sub_data[[group_col]] == pair[2]]
  observed_diff <- mean(group2_vec) - mean(group1_vec)
  
  test_result <- coin::oneway_test(
    as.formula(paste(value_col, "~", group_col)),
    data = sub_data,
    alternative = "less", 
    distribution = coin::approximate(nresample = n_perm)
  )
  p_value <- coin::pvalue(test_result)
  
  list(
    comparison = paste(pair[2], "vs", pair[1]),
    mean_diff = observed_diff,
    p_value = as.numeric(p_value),
    statistic = coin::statistic(test_result)
  )
}

results <- lapply(comparison_pairs, function(pair) {
  run_coin_test(data_id, "identity_4level", "play_count", pair)
}) %>% bind_rows()

print(results)
```

# 13. 发布时间窗口的流量效应分析(模块十)

为了探究 “发布时机（Timing）” 对歌单最终播放量的影响，我们对时间戳数据进行了多维度的聚合分析（Temporal Aggregation），分别考察了 周度循环（Day of Week） 和 月度循环（Month of Year） 下的平均播放量差异

1. 周度效应：周末的“休闲红利”
统计显示，周日（Sunday） 发布的歌单拥有全周最高的平均播放量，显著高于工作日（周一至周五）
我们推测在周末，用户拥有完整的整块休闲时间进行深度浏览和沉浸式聆听，此时发布的歌单更容易获得高完播率和互动，从而触发算法的初始推荐池。相比之下，工作日发布的歌单容易淹没在碎片化信息流中

2. 季节效应：特定月份的流量峰值
在月度维度上，9月发布的歌单显示出异常高的平均流量
这可能与特定的情绪周期有关。9月通常对应“开学季”或“初秋”，是情感波动较大且功能性音乐需求（如学习背景音、秋日情绪歌单）激增的节点

```{r module_10_time}
#====模块10：时序分析====
###最佳发布时间分析
# 1. 数据准备
data_time_week <- data_cleaned %>%
  mutate(
    create_dt = as_datetime(create_time),
    day_of_week = factor(wday(create_dt, label = TRUE, week_start = 1)) 
  ) %>%
  group_by(day_of_week) %>%
  summarise(avg_play = mean(play_count, na.rm = TRUE))

# 绘图：独立时间柱状图
ggplot(data_time_week, aes(x = day_of_week, y = avg_play, fill = day_of_week)) +
  geom_col(alpha = 0.8) +
  scale_y_continuous(labels = comma) + 
  scale_fill_brewer(palette = "Blues") +
  labs(title = "不同发布时间的流量差异", x = "发布时间", y = "平均播放量") +
  theme_minimal() +
  theme(legend.position = "none")

# 月份分析
data_time_month <- data_cleaned %>%
  mutate(
    create_dt = as_datetime(create_time),
    month_of_year = month(create_dt, label = TRUE, abbr = TRUE) 
  ) %>%
  filter(!is.na(month_of_year)) %>%
  group_by(month_of_year) %>%
  summarise(avg_play = mean(play_count, na.rm = TRUE)) %>%
  ungroup()

ggplot(data_time_month, aes(x = month_of_year, y = avg_play)) +
  geom_col(aes(fill = avg_play), alpha = 0.9) +
  scale_fill_distiller(palette = "Blues", direction = 1) +
  scale_y_continuous(labels = comma) +
  labs(title = "不同发布月份的流量差异", x = "发布时间", y = "平均播放量") +
  theme_minimal() +
  theme(legend.position = "none")
```
# 14.

```{r module_11}

```

# 15. 基于 PCA-神经网络的流量预测模型评估 (模块十二)

为了验证“播放量是否可被预测”，我们构建了一个结合了 主成分分析（PCA） 与 神经网络（Neural Network） 的混合预测模型。该模型利用 PCA 提取歌单与作者的潜在特征（Latent Features），并结合高权重的互动指标，对测试集数据进行了盲测。以下是针对模型表现的详细诊断

**分析结论：**

1.特征工程的有效性
模型引入了 PC1 至 PC4 四个主成分。这有效地解决了原始数据中“作者等级”、“歌单数”、“粉丝数”之间存在的 多重共线性 问题，同时保留了原始变量的主要信息量。这种降维处理显著提升了神经网络训练的收敛速度和泛化能力，避免了过拟合

2.模型拟合优度模型的决定系数 $R^2 = 0.741$
这证实了歌单的流量表现并非完全随机，而是高度依赖于“内容属性（PCA特征）”与“互动反馈（互动指标）”的确定性组合

3.商业应用价值：流量分级预测
由Prediction Error Matrix（混淆矩阵）来看，我们的模型可以进行 “流量层级分类（Tier Classification）”操作
高准确率区域：矩阵左下角显示，在“Low”（低流量）类别中，模型正确预测了 409
个样本，误判极少。这说明模型极擅长识别“非爆款”内容
通过准确剔除低潜力内容（Low Tier），运营团队可以将宝贵的首页曝光位和推广预算集中在模型预测为“Medium”或“High”的内容上，从而极大优化流量分发效率
```{r module_12_neural_network, fig.width=8, fig.height=6}
# ====模块12：神经网络预测以及error矩阵====

nn_data <- data_cleaned %>%
  dplyr::select(
    play_count, 
    collect_count, share_count, comment_count, # 中间层/强相关变量
    fans, grade, playlists,                # 作者特征
    length_name, length_intro, number_songs, number_hot_singers # 歌单特征
  ) %>%
  na.omit()

# 2. PCA分析
pca_features <- nn_data %>% 
  dplyr::select(fans, grade, playlists, length_name, length_intro, number_songs, number_hot_singers)

pca_res <- prcomp(pca_features, scale. = TRUE)
pca_scores <- as.data.frame(pca_res$x[, 1:4])
names(pca_scores) <- c("PC1", "PC2", "PC3", "PC4")

# 数据合并
model_data <- cbind(
  dplyr::select(nn_data, play_count, collect_count, share_count, comment_count), 
  pca_scores
)

# 归一化/标准化数据
preproc_values <- preProcess(model_data, method = c("center", "scale"))
model_data_scaled <- predict(preproc_values, model_data)

# 划分训练集和测试集
set.seed(123)
train_index <- createDataPartition(model_data_scaled$play_count, p = 0.7, list = FALSE)
train_set <- model_data_scaled[train_index, ]
test_set <- model_data_scaled[-train_index, ]

# 神经网络模型
nn_formula <- play_count ~ collect_count + share_count + comment_count + PC1 + PC2 + PC3 + PC4
nn_model <- nnet(nn_formula, data = train_set, size = 10, decay = 0.01, linout = TRUE, maxit = 500, trace = FALSE)

# 预测与评估
predictions_scaled <- predict(nn_model, test_set)

# 反归一化
play_count_mean <- mean(model_data$play_count)
play_count_sd <- sd(model_data$play_count)
predictions_raw <- (predictions_scaled * play_count_sd) + play_count_mean
test_set$play_count_raw <- (test_set$play_count * play_count_sd) + play_count_mean

rmse_val <- sqrt(mean((predictions_raw - test_set$play_count_raw)^2))
r2_val <- cor(predictions_raw, test_set$play_count_raw)^2

cat("\n=== 神经网络模型评估 ===\n")
cat("RMSE:", rmse_val, "\n")
cat("R-squared:", r2_val, "\n")

# 绘制预测值 vs 真实值
pred_df <- data.frame(Actual = test_set$play_count_raw, Predicted = predictions_raw)

ggplot(pred_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "purple") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Neural Network Prediction Accuracy",
    subtitle = paste("R-squared =", round(r2_val, 3)),
    x = "Actual Play Count",
    y = "Predicted Play Count"
  ) +
  theme_minimal()

# 误差矩阵可视化
breaks <- c(0, 793134.5, 2269060.5, 4575695, 32119005)
actual_class <- cut(test_set$play_count_raw, breaks = breaks, labels = c("Low", "Medium", "High", "Very High"), include.lowest = TRUE)
pred_class <- cut(predictions_raw, breaks = breaks, labels = c("Low", "Medium", "High", "Very High"), include.lowest = TRUE)

error_matrix <- table(Predicted = pred_class, Actual = actual_class)
melted_cmat <- melt(error_matrix)

ggplot(data = melted_cmat, aes(x = Actual, y = Predicted, fill = value)) +
  geom_tile() +
  geom_text(aes(label = value), color = "white", size = 5) +
  scale_fill_gradient(low = "#3498DB", high = "#E74C3C") +
  labs(title = "Prediction Error Matrix") +
  theme_minimal()
```

# 16. 总结与建议

本研究通过对 2048 个网易云音乐歌单样本的全方位挖掘，从基础统计到机器学习预测，成功拆解了播放量飙升背后的“隐藏公式”。特别是最终构建的 PCA-神经网络混合模型，以 $R^2 = 0.741$ 的高拟合优度证实了歌单流量并非不可捉摸的“玄学”，而是内容属性、互动反馈与创作者权益共同作用的确定性结果。
基于上述数据分析，针对不同利益相关方提出以下建议：

* **对于平台方**
实施“负向剔除”资源优化策略：
利用本报告验证的 PCA-神经网络模型，平台不应只关注预测“谁是爆款”，而应利用模型极擅长识别“非爆款”（Low Tier）的特性，准确剔除低潜力内容。将原本均匀撒向海量新歌单的首页曝光位和推广预算，从预测为“Low”的 40% 内容中抽离出来，集中火力“饱和式攻击”模型预测为 “Medium” 或 “High” 的腰部潜力股，从而极大优化流量分发效率。
扶持“功能性”内容：
数据显示，读书、健身等场景化歌单具有极高的日均播放效率和长尾生命力。平台应建立专门的场景化标签索引体系，将其作为稳定的流量基石

* **对于创作者**
SEO 与标签策略：
放弃“标题党”或过于文艺晦涩的命名，增加标题和简介的长度（Text Density）以覆盖更多搜索关键词
在标签选择上，采取 “2+1”策略：2 个热门标签（如华语、流行）确保基础曝光池，1 个垂直标签（如治愈、学习）精准定位受众

择时与身份运营：
利用 周日（休闲红利） 和 9月（季节性情绪） 的时间窗口发布重点内容
停止盲目堆砌歌单数量，转而通过提升单品质量（尤其是诱导收藏行为）来争取 “达人” 身份，这是比单纯积累粉丝更有效的流量杠杆

* **对于用户**
在挑选歌单时，不要迷信作者的“认证”光环或粉丝总量。应更多关注歌单的 “收藏/播放比” 以及作者的 “达人” 标识，这些往往是高质量内容的真实信号

